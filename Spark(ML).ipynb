{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75318950-c301-4ac6-adec-dea0bbc316f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/06/03 00:20:55 WARN Utils: Your hostname, umair, resolves to a loopback address: 127.0.1.1; using 192.168.100.25 instead (on interface wlp2s0)\n",
      "25/06/03 00:20:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/03 00:20:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, length, when\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Amazon Review Big Data ML\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcc5dae-674e-406a-9c81-0a78ab59663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd parquet_chunks && pwd && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a42dfc4-1df4-49dc-88cb-af69ebade734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/03 00:21:12 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: parquet_chunks/*.parquet.\n",
      "java.io.FileNotFoundException: File parquet_chunks/*.parquet does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Load all chunks\n",
    "df = spark.read.parquet(\"parquet_chunks/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bad9b5c-8e6e-435a-a28b-dac229a02632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Schema:\n",
      "root\n",
      " |-- rating: double (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- attachment_type: string (nullable = true)\n",
      " |    |    |-- large_image_url: string (nullable = true)\n",
      " |    |    |-- medium_image_url: string (nullable = true)\n",
      " |    |    |-- small_image_url: string (nullable = true)\n",
      " |-- asin: string (nullable = true)\n",
      " |-- parent_asin: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- helpful_vote: long (nullable = true)\n",
      " |-- verified_purchase: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"✅ Schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97fdeeee-e658-4e62-adb0-63440bfbb02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 4) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Total records: 1500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"🔢 Total records:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b64733-9c0a-4c2e-b4e3-80192ecc7238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+----------+-----------+--------------------+-------------+------------+-----------------+\n",
      "|rating|               title|                text|              images|      asin|parent_asin|             user_id|    timestamp|helpful_vote|verified_purchase|\n",
      "+------+--------------------+--------------------+--------------------+----------+-----------+--------------------+-------------+------------+-----------------+\n",
      "|   1.0|Not a watercolor ...|It is definitely ...|[{IMAGE, https://...|B09BGPFTDB| B09BGPFTDB|AFKZENTNBQ7A7V7UX...|1642399598485|           0|             true|\n",
      "|   5.0|Updated: after 1s...|Updated: after fi...|                  []|0593235657| 0593235657|AFKZENTNBQ7A7V7UX...|1640629604904|           1|             true|\n",
      "|   5.0|Excellent! I love...|I bought it for t...|                  []|1782490671| 1782490671|AFKZENTNBQ7A7V7UX...|1640383495102|           0|             true|\n",
      "|   5.0|Updated after 1st...|Updated: after 1s...|                  []|0593138228| 0593138228|AFKZENTNBQ7A7V7UX...|1640364906602|           0|            false|\n",
      "|   5.0| Beautiful patterns!|I love this book!...|[{IMAGE, https://...|0823098079| 0823098079|AFKZENTNBQ7A7V7UX...|1637312253230|           0|             true|\n",
      "+------+--------------------+--------------------+--------------------+----------+-----------+--------------------+-------------+------------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"📊 Sample:\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd6c5748-7e33-45ef-bada-73621697c034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----+\n",
      "|rating|text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |textLength|label|\n",
      "+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----+\n",
      "|1.0   |It is definitely not a watercolor book.  The paper bucked completely.  The pages honestly appear to be photo copies of other pictures. I say that bc if you look at the seal pics you can see the tell tale line at the bottom of the page.  As someone who has made many photocopies of pages in my time so I could try out different colors & mediums that black line is a dead giveaway to me. It’s on other pages too.  The entire book just seems off. Nothing is sharp & clear. There is what looks like toner dust on all the pages making them look muddy.  There are no sharp lines & there is no clear definition.  At least there isn’t in my copy.  And the Coloring Book for Adult on the bottom of the front cover annoys me. Why is it singular & not plural?  They usually say coloring book for kids or coloring book for kids & adults or coloring book for adults- plural.  Lol  Plus it would work for kids if you can get over the grey scale nature of it.  Personally I’m not going to waste expensive pens & paints trying to paint over the grey & black mess.  I grew up in SW Florida minutes from the beaches & I was really excited about the sea life in this. I hope the printers & designers figure out how to clean up the mess bc some of the designs are really cute. They just aren’t worth my time to hand trace & transfer them, but I’m sure there are ppl that will be up to the challenge.  This is one is a hard no. Going back. I tried.|1427      |0    |\n",
      "|5.0   |Updated: after first book arrived very damaged the replacement arrived in perfect condition.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |92        |1    |\n",
      "|5.0   |I bought it for the bag on the front so it paid for itself with that imo.  I haven’t started anything yet from it bc I’m still busy with other projects, but I’m really looking forward to starting some.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |201       |1    |\n",
      "|5.0   |Updated: after 1st arrived damaged the replacement arrived in perfect condition.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |80        |1    |\n",
      "|5.0   |I love this book!  The patterns are lovely. I admit I’m not even following all of them to the letter. I am more of a free spirit.  I see the designs & take a look at the dimensions & just kind of wing it from there sometimes. lol. I’m 54 now & much less rigid in my arts & crafts than I was in my younger days when I would go to 6 stores trying to find the right threads or yarns. Lol. Those days are over!  Praise the Lord!  It’s a lot more fun now too.  As Bob Ross used to say- “Happy Mistakes!”  I make a lot of them sometimes, but I have found that the journey for me is a lot more interesting these days. Lol.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |615       |1    |\n",
      "+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Clean and transform the data\n",
    "df_clean = (\n",
    "    df.filter(col(\"text\").isNotNull() & col(\"rating\").isNotNull())\n",
    "      .withColumn(\"textLength\", length(col(\"text\")))\n",
    "      .withColumn(\"label\", when(col(\"rating\") >= 4, 1).otherwise(0))  # Binary sentiment label\n",
    ")\n",
    "\n",
    "# Optional: check sample rows\n",
    "df_clean.select(\"rating\", \"text\", \"textLength\", \"label\").show(5, truncate=False)\n",
    "\n",
    "# Save cleaned data for ML\n",
    "df_clean.write.mode(\"overwrite\").parquet(\"cleaned_reviews.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9d73ed0-93e0-45d2-b4cb-009d2764fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edea1b3e-5986-4d65-9e1b-48251e09f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload cleaned data\n",
    "df = spark.read.parquet(\"cleaned_reviews.parquet\")\n",
    "\n",
    "# Sample to avoid OOM (adjust as needed)\n",
    "df = df.limit(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd06b50f-44a1-49c9-ad23-8e7a63ed3a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Correct column names used here\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "tf = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=5000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Build ML pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, tf, idf, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e4f0806-a3a0-43d6-a91f-5070895e1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cff5a98-5648-48b1-b4ff-bcde4622bee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/03 00:22:49 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/06/03 00:24:21 WARN MemoryStore: Not enough space to cache rdd_56_0 in memory! (computed 113.1 MiB so far)\n",
      "25/06/03 00:24:21 WARN BlockManager: Persisting block rdd_56_0 to disk instead.\n",
      "[Stage 110:=============================================>           (4 + 1) / 5]"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34f1d115-4539-41bd-ae42-307098b5f8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ffeb3c1-243e-452c-8645-0436cea38ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 AUC: 0.8708\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"🎯 AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74231330-d981-4616-abec-c6f2d5da9c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Amazon Reviews Kernel",
   "language": "python",
   "name": "amazon_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
